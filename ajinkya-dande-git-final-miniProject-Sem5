import cv2
from matplotlib import pyplot as plt
from transform import four_point_transform
#from pyimagesearch import imutils
#from skimage.filter import threshold_adaptive
import numpy as np
import argparse
import urllib
# construct the argument parser and parse the arguments
#ap = argparse.ArgumentParser()
#ap.add_argument("-i", "--image", required = True,
#	help = "Path to theq image to be scanned")
#args = vars(ap.parse_args())
# load the image and compute the ratio of the old height
# to the new height, clone it, and resize it


stream = urllib.urlopen('http://192.168.43.88:8080/video')
bytes = ''
print stream
while True:
	bytes += stream.read(1024)
	a = bytes.find('\xff\xd8')
	b = bytes.find('\xff\xd9')
	if a!=-1 and b!=-1:
		#print stream
        #print bytes,"\n","a ",a,"    b ",b
        jpg = bytes[a:b+2]
        bytes = bytes[b+2:]
        i = cv2.imdecode(np.fromstring(jpg, dtype=np.uint8),1)
        cv2.imshow('i',i)
        if cv2.waitKey(1)&0xff == ord('q'):    
            break;
cv2.destroyAllWindows()
#cap.release()

image = i
orig = image.copy()
#image = imutils.resize(image, height = 500)

r = 500.0/ image.shape[1]
dim = (500, int(image.shape[0] * r))
image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)
ratio = image.shape[0] / 500.0


orig = image.copy()
# convert the image to grayscale, blur it, and find edges
# in the image
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (5, 5), 0)
edged = cv2.Canny(gray, 75, 200)	#75,200

# show the original image and the edge detected image
print "STEP 1: Edge Detection"
cv2.imshow("Image", image)
cv2.imshow("Edged", edged)
cv2.waitKey(0)
cv2.destroyAllWindows()
# find the contours in the edged image, keeping only the
# largest ones, and initialize the screen contour
(_,cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]
#print "cnts: ", cnts
screenCnt = 0 
# loop over the contours
n = .02
flag = True
while(n<.5 and flag==True):		#remove while loop if wrong contour is being detected
        #print n
        for c in cnts:
                # approximate the contour
                peri = cv2.arcLength(c, True)
                approx = cv2.approxPolyDP(c,n*peri, True)
                #print "Approx: ", len(approx)
        
                # if our approximated contour has four points, then we
                # can assume that we have found our screen
                if len(approx) == 4:
                        screenCnt = approx
                        flag=False
                        break
        n+=.01
        
print "Screen Count is ",screenCnt
# show the contour (outline) of the piece of paper
print "STEP 2: Find contours of paper"
cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)
cv2.imshow("Outline", image)
cv2.waitKey(0)
cv2.destroyAllWindows()
# apply the four point transform to obtain a top-down
# view of the original image

warped = four_point_transform(orig, screenCnt.reshape(4, 2))
#warped = orig[ screenCnt[0][0][1]:screenCnt[1][0][1],screenCnt[0][0][0]:screenCnt[2][0][0]]
cv2.imshow('Warped',warped)
cv2.waitKey(0)
cv2.destroyAllWindows()
# convert the warped image to grayscale, then threshold it
# to give it that 'black and white' paper effect
#warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)
#warped = threshold_adaptive(warped, 250, offset = 10)
#warped = warped.astype("uint8") * 255

# show the original and scanned images


r = 500.0/warped.shape[1]
dim = (500, 240)
warped = cv2.resize(warped, dim, interpolation = cv2.INTER_AREA)

print "STEP 3: Apply perspective transform"
#cv2.imshow("Original", image)
cv2.imshow("Orignal", orig)
cv2.imshow("Scanned", warped)
cv2.waitKey(0)
cv2.destroyAllWindows()
#cv2.imwrite('100t.png',warped)

#'50t.png','100t.png',
l=['20t.png','20t2.png','100.jpg','101.jpg','500.jpg','500t.png','1001.png','1002.png','1000t.png','1000t2.png']
maxVal=10
maxPt=-1
maxKp=0
orb = cv2.ORB_create()
img1 = warped # queryImage
#img1 = cv2.imread(l[4])     
kp1, des1 = orb.detectAndCompute(img1,None)
for i in range (0,len(l)):
	#print l[i]
	
	img2 = cv2.imread(l[i]) # trainImag
	#cv2.imshow("Input Image", img1)
	#cv2.imshow("Stored image", img2)
	#cv2.waitKey(0)
	#cv2.imwrite('10x.png',img1)
	#cv2.destroyAllWindows()        
	# Initiate SIFT detector

	# find the keypoints and descriptors with SIFT
	kp2, des2 = orb.detectAndCompute(img2,None)
	# create BFMatcher object
	#bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
	# Match descriptors.
	#matches = bf.match(des1,des2)

	# Sort them in the order of their distance.
	#matches = sorted(matches, key = lambda x:x.distance) 
	# Draw first 10 matches.
	#img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:50],None, flags=2)

	#plt.imshow(img3),plt.show()
	bf = cv2.BFMatcher()
	matches = bf.knnMatch(des1,des2, k=2)

	# Apply ratio test
	good = []
	for m,n in matches:
		if (m.distance < 0.75*n.distance):
			good.append([m])

	if(len(good)>maxVal):
		maxVal=len(good)
		maxPt=i
		maxKp=kp2
		#print good
	print i,' ',l[i],' ',len(good)
	# cv2.drawMatchesKnn expects list of lists as matches.
if(maxVal!=10):
    print l[maxPt]
    print 'good matches ',maxVal
    img2=cv2.imread(l[maxPt])
    '''cv2.imshow("Input Image", img1)
    cv2.imshow("Stored image", img2)
    cv2.waitKey(0)
    cv2.destroyAllWindows() '''
    img3 = cv2.drawMatchesKnn(img1,kp1,img2,maxKp,good,2)
    plt.imshow(img3),plt.show()
else:
    print 'No Matches'
	
#plt.hide()
#	if(cv2.waitKey(1)&0xff==ord('q')):
#               break;

#cv2.destroyAllWindows()
